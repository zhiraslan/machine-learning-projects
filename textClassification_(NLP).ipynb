{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset of sms messages\n",
    "df = pd.read_table(\"SMSSpamCollection\", header=None, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       5572 non-null   object\n",
      " 1   1       5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n",
      "None\n",
      "           0                       1\n",
      "count   5572                    5572\n",
      "unique     2                    5169\n",
      "top      ham  Sorry, I'll call later\n",
      "freq    4825                      30\n"
     ]
    }
   ],
   "source": [
    "# print useful information about the dataset\n",
    "\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     4825\n",
      "spam     747\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check the class distribution\n",
    "classes = df[0]\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ham': 0, 'spam': 1}\n"
     ]
    }
   ],
   "source": [
    "# переведем названия классов в бинарный формат: 0 = ham, 1 = spam\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(classes)\n",
    "\n",
    "# преобразуем в словарь, чтобы убедиться в однозначном соответствии меток классов и их начальных подписей\n",
    "print(dict(zip(df.iloc[:,0][:10], Y[:10])))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>spam</td>\n",
       "      <td>Girls around you want to have a sex. Follow th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>spam</td>\n",
       "      <td>Play chess with world champions! Use all the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5574</th>\n",
       "      <td>spam</td>\n",
       "      <td>'Pikelny club fans to tickle point' Hello! Las...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                                                  1\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "5572  spam  Girls around you want to have a sex. Follow th...\n",
       "5573  spam  Play chess with world champions! Use all the p...\n",
       "5574  spam  'Pikelny club fans to tickle point' Hello! Las..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5572] = \"spam\", \"Girls around you want to have a sex. Follow the link to see Yilia’s message 'Honey, let’s go out and...'\"\n",
    "df.loc[5573] = \"spam\", \"Play chess with world champions! Use all the potential of the app and take a chance to play with best world players! Follow the link to\"\n",
    "df.loc[5574] = \"spam\", \"'Pikelny club fans to tickle point' Hello! Last month, you purchased goods for the amount of ₽5648.73, accumulated 89 points. Have time to use them on black Friday!\"\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь необходимо избавиться от ненужной информации. Что она из себя представляет?\n",
    "Например, почтовые адреса. Классификатору не важно, какая именно электронная почта указана в письме; важно, что она просто есть. Если мы оставим адреса в письмах (адреса, вероятнее всего, у всех разные), мы будем обладать теми признаками каждого отдельного письма, которые не присущи всем остальным. Это нам ничего не даст - напротив, лучше те места, которые различны у всех, сделать одинаковыми, чтобы не при обучении не отвлекаться на неиформативные отличия.\n",
    "Примерами подобных 'лишних' элементов в письмах помимо email-адресов являются также web-адреса, номера телефонов, знаки валют, знаки пунктуации. Будем заменять их конкретные значения на обобщающие слова (например, **'svolkov@mail.ru'** - **'emailaddress'**) с помощью регулярных выражений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Go until jurong point, crazy.. Available only ...\n",
      "1                        Ok lar... Joking wif u oni...\n",
      "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3    U dun say so early hor... U c already then say...\n",
      "4    Nah I don't think he goes to usf, he lives aro...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "text_messages = df[1]\n",
    "print(text_messages[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "processed = text_messages\n",
    "for i in range(len(processed)):\n",
    "    # почта\n",
    "    processed[i] = re.sub(r'^.+@[^\\.].*\\.[a-z]{2,}$', 'emailaddress', processed[i])\n",
    "    # URL\n",
    "    processed[i] = re.sub(r'^http\\://[a-zA-Z8-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$', 'webaddress', processed[i])\n",
    "    # денежные символы\n",
    "    processed[i] = re.sub(r'[$€£₽]', 'moneysym', processed[i])\n",
    "    # номера телефонов\n",
    "    processed[i] = re.sub(r'[+]{,1}[(0-9)-]{11,17}', 'phonenum', processed[i])\n",
    "    # знаки пунктуации\n",
    "    processed[i] = re.sub(r'[^\\w\\s\\d]', '', processed[i])\n",
    "    # пробелы между токенами\n",
    "    processed[i] = re.sub(r'\\s+', ' ', processed[i])\n",
    "    # пробелы, символы табуляции\n",
    "    processed[i] = processed[i].strip()\n",
    "#     замена цифр\n",
    "#     processed[i] = re.sub(r'\\d+(\\.\\d+)?', 'numbr', processed[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    go until jurong point crazy available only in ...\n",
      "1                              ok lar joking wif u oni\n",
      "2    free entry in 2 a wkly comp to win fa cup fina...\n",
      "3          u dun say so early hor u c already then say\n",
      "4    nah i dont think he goes to usf he lives aroun...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "processed = processed.str.lower()\n",
    "print(processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# удаляем стоп-слова\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "processed = processed.apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       go jurong point crazi avail bugi n great world...\n",
       "1                                   ok lar joke wif u oni\n",
       "2       free entri 2 wkli comp win fa cup final tkt 21...\n",
       "3                     u dun say earli hor u c alreadi say\n",
       "4               nah dont think goe usf live around though\n",
       "                              ...                        \n",
       "5570    guy bitch act like id interest buy someth els ...\n",
       "5571                                       rofl true name\n",
       "5572    girl around want sex follow link see yilia mes...\n",
       "5573    play chess world champion use potenti app take...\n",
       "5574    pikelni club fan tickl point hello last month ...\n",
       "Name: 1, Length: 5575, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# находим основы слов (осуществим стемминг методом Портера)\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "processed = processed.apply(lambda x: ' '.join(ps.stem(term) for term in x.split()))\n",
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_words = []\n",
    "for message in processed:\n",
    "    words = word_tokenize(message)\n",
    "    for word in words:\n",
    "        all_words.append(word)\n",
    "    \n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)  # список кортежей с токенами и их частотами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7897"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u',\n",
       " 'call',\n",
       " '2',\n",
       " 'im',\n",
       " 'go',\n",
       " 'get',\n",
       " 'phonenum',\n",
       " 'ur',\n",
       " 'come',\n",
       " 'dont',\n",
       " '4',\n",
       " 'ok',\n",
       " 'ltgt',\n",
       " 'free',\n",
       " 'know',\n",
       " 'like',\n",
       " 'got',\n",
       " 'love',\n",
       " 'want',\n",
       " 'ill',\n",
       " 'day',\n",
       " 'time',\n",
       " 'good',\n",
       " 'text',\n",
       " 'send',\n",
       " 'need',\n",
       " 'one',\n",
       " 'txt',\n",
       " 'see',\n",
       " 'today',\n",
       " 'ü',\n",
       " 'think',\n",
       " 'home',\n",
       " 'take',\n",
       " 'lor',\n",
       " 'stop',\n",
       " 'repli',\n",
       " 'tell',\n",
       " 'sorri',\n",
       " 'still',\n",
       " 'r',\n",
       " 'back',\n",
       " 'mobil',\n",
       " 'make',\n",
       " 'n',\n",
       " 'phone',\n",
       " 'say',\n",
       " 'new',\n",
       " 'work',\n",
       " 'pleas',\n",
       " 'well',\n",
       " 'week',\n",
       " 'later',\n",
       " 'hi',\n",
       " 'da',\n",
       " 'ask',\n",
       " 'miss',\n",
       " 'cant',\n",
       " 'hope',\n",
       " 'meet',\n",
       " 'happi',\n",
       " 'night',\n",
       " 'tri',\n",
       " 'give',\n",
       " 'claim',\n",
       " 'wait',\n",
       " 'thing',\n",
       " 'oh',\n",
       " 'much',\n",
       " 'great',\n",
       " 'hey',\n",
       " 'pl',\n",
       " 'dear',\n",
       " 'wat',\n",
       " 'messag',\n",
       " 'number',\n",
       " 'na',\n",
       " 'friend',\n",
       " 'thank',\n",
       " 'that',\n",
       " 'way',\n",
       " 'prize',\n",
       " 'right',\n",
       " 'feel',\n",
       " 'let',\n",
       " 'msg',\n",
       " 'wan',\n",
       " 'even',\n",
       " 'pick',\n",
       " 'alreadi',\n",
       " 'tomorrow',\n",
       " 'said',\n",
       " 'ye',\n",
       " 'realli',\n",
       " 'yeah',\n",
       " 'min',\n",
       " 'e',\n",
       " 'amp',\n",
       " 'leav',\n",
       " 'care',\n",
       " 'co',\n",
       " 'didnt',\n",
       " 'babe',\n",
       " '1',\n",
       " 'morn',\n",
       " 'win',\n",
       " 'last',\n",
       " 'c',\n",
       " 'life',\n",
       " 'sure',\n",
       " 'servic',\n",
       " 'ive',\n",
       " 'anyth',\n",
       " 'would',\n",
       " 'keep',\n",
       " 'cash',\n",
       " 'find',\n",
       " 'year',\n",
       " 'contact',\n",
       " 'buy',\n",
       " 'sleep',\n",
       " 'use',\n",
       " 'lol',\n",
       " 'tone',\n",
       " 'look',\n",
       " 'everi',\n",
       " 'k',\n",
       " 'nokia',\n",
       " 'start',\n",
       " 'smile',\n",
       " 'wish',\n",
       " 'also',\n",
       " 'watch',\n",
       " 'someth',\n",
       " 'show',\n",
       " 'sent',\n",
       " 'finish',\n",
       " 'end',\n",
       " 'award',\n",
       " '3',\n",
       " 'b',\n",
       " 'urgent',\n",
       " 'place',\n",
       " 'gud',\n",
       " 'us',\n",
       " 'guy',\n",
       " 'around',\n",
       " 'custom',\n",
       " 'tonight',\n",
       " 'next',\n",
       " 'first',\n",
       " 'person',\n",
       " 'talk',\n",
       " 'someon',\n",
       " 'went',\n",
       " 'could',\n",
       " 'gon',\n",
       " 'soon',\n",
       " 'collect',\n",
       " 'chat',\n",
       " 'mani',\n",
       " 'per',\n",
       " 'help',\n",
       " 'late',\n",
       " 'plan',\n",
       " 'live',\n",
       " 'alway',\n",
       " 'nice',\n",
       " 'money',\n",
       " 'word',\n",
       " 'wont',\n",
       " 'ya',\n",
       " 'minut',\n",
       " 'dun',\n",
       " 'special',\n",
       " 'check',\n",
       " 'your',\n",
       " 'told',\n",
       " '16',\n",
       " 'tc',\n",
       " 'name',\n",
       " 'v',\n",
       " 'mean',\n",
       " 'lot',\n",
       " 'hour',\n",
       " 'x',\n",
       " 'hello',\n",
       " 'reach',\n",
       " 'girl',\n",
       " 'peopl',\n",
       " 'guarante',\n",
       " 'shop',\n",
       " 'yet',\n",
       " 'happen',\n",
       " 'thk',\n",
       " 'done',\n",
       " 'play',\n",
       " 'may',\n",
       " 'thought',\n",
       " 'havent',\n",
       " 'haha',\n",
       " 'class',\n",
       " 'best',\n",
       " 'offer',\n",
       " 'fuck',\n",
       " 'receiv',\n",
       " 'bit',\n",
       " 'line',\n",
       " 'fine',\n",
       " 'lunch',\n",
       " 'eat',\n",
       " 'how',\n",
       " 'man',\n",
       " 'never',\n",
       " 'job',\n",
       " 'heart',\n",
       " 'month',\n",
       " 'stuff',\n",
       " 'car',\n",
       " 'mayb',\n",
       " 'draw',\n",
       " 'holiday',\n",
       " 'enjoy',\n",
       " 'yup',\n",
       " '18',\n",
       " 'account',\n",
       " '5',\n",
       " 'chanc',\n",
       " 'sm',\n",
       " 'cool',\n",
       " 'long',\n",
       " 'drive',\n",
       " 'guess',\n",
       " 'better',\n",
       " 'dat',\n",
       " 'readi',\n",
       " 'god',\n",
       " 'mind',\n",
       " 'pay',\n",
       " 'worri',\n",
       " 'problem',\n",
       " 'latest',\n",
       " 'cost',\n",
       " 'wonder',\n",
       " 'weekend',\n",
       " 'room',\n",
       " 'luv',\n",
       " 'boy',\n",
       " 'bring',\n",
       " 'quit',\n",
       " 'world',\n",
       " 'lar',\n",
       " 'date',\n",
       " 'half',\n",
       " 'box',\n",
       " 'noth',\n",
       " 'hous',\n",
       " 'book',\n",
       " '1st',\n",
       " 'yo',\n",
       " 'anoth',\n",
       " 'big',\n",
       " 'voucher',\n",
       " 'game',\n",
       " 'select',\n",
       " 'camera',\n",
       " 'charg',\n",
       " 'sweet',\n",
       " 'real',\n",
       " 'birthday',\n",
       " 'landlin',\n",
       " 'stay',\n",
       " 'shit',\n",
       " 'kiss',\n",
       " 'put',\n",
       " 'point',\n",
       " 'speak',\n",
       " 'dinner',\n",
       " 'moneysym1000',\n",
       " 'po',\n",
       " 'join',\n",
       " 'sir',\n",
       " 'liao',\n",
       " 'ju',\n",
       " 'ever',\n",
       " 'id',\n",
       " 'xxx',\n",
       " 'rememb',\n",
       " '150ppm',\n",
       " 'what',\n",
       " 'might',\n",
       " 'actual',\n",
       " 'final',\n",
       " 'appli',\n",
       " 'earli',\n",
       " 'he',\n",
       " 'di',\n",
       " 'hear',\n",
       " 'hurt',\n",
       " 'chang',\n",
       " 'aight',\n",
       " 'question',\n",
       " 'two',\n",
       " 'probabl',\n",
       " 'pic',\n",
       " 'fun',\n",
       " 'network',\n",
       " 'run',\n",
       " 'part',\n",
       " 'pa',\n",
       " 'bed',\n",
       " 'rate',\n",
       " 'answer',\n",
       " 'video',\n",
       " 'babi',\n",
       " 'den',\n",
       " 'princess',\n",
       " '6',\n",
       " 'left',\n",
       " 'rington',\n",
       " 'forgot',\n",
       " 'anyway',\n",
       " 'moneysym2000',\n",
       " 'easi',\n",
       " 'walk',\n",
       " 'thanx',\n",
       " 'wake',\n",
       " 'made',\n",
       " 'dunno',\n",
       " 'orang',\n",
       " 'bad',\n",
       " 'code',\n",
       " 'there',\n",
       " '2nd',\n",
       " 'frnd',\n",
       " 'ah',\n",
       " 'littl',\n",
       " 'everyth',\n",
       " 'dad',\n",
       " 'enough',\n",
       " 'bu',\n",
       " 'pain',\n",
       " 'school',\n",
       " 'leh',\n",
       " 'face',\n",
       " 'bore',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'mate',\n",
       " 'pound',\n",
       " 'doesnt',\n",
       " 'afternoon',\n",
       " 'dream',\n",
       " 'without',\n",
       " 'tv',\n",
       " 'xma',\n",
       " 'tmr',\n",
       " 'sound',\n",
       " 'g',\n",
       " 'lose',\n",
       " 'read',\n",
       " 'movi',\n",
       " 'sat',\n",
       " '7',\n",
       " 'detail',\n",
       " 'gift',\n",
       " 'await',\n",
       " 'wif',\n",
       " 'moneysym150',\n",
       " '9',\n",
       " 'credit',\n",
       " 'decid',\n",
       " 'sinc',\n",
       " 'came',\n",
       " 'wk',\n",
       " 'test',\n",
       " 'must',\n",
       " 'mail',\n",
       " 'sexi',\n",
       " 'post',\n",
       " 'town',\n",
       " 'entri',\n",
       " 'goe',\n",
       " 'though',\n",
       " 'set',\n",
       " 'uk',\n",
       " 'lesson',\n",
       " 'abt',\n",
       " 'okay',\n",
       " 'bt',\n",
       " 'smoke',\n",
       " 'abl',\n",
       " 'hav',\n",
       " 'import',\n",
       " 'true',\n",
       " 'offic',\n",
       " 'updat',\n",
       " 'mob',\n",
       " 'wen',\n",
       " 'price',\n",
       " 'juz',\n",
       " 'enter',\n",
       " 'bath',\n",
       " 'til',\n",
       " 'plz',\n",
       " 'poli',\n",
       " 'drink',\n",
       " 'away',\n",
       " 'plu',\n",
       " 'colour',\n",
       " 'till',\n",
       " 'hair',\n",
       " 'els',\n",
       " 'top',\n",
       " 'hand',\n",
       " 'order',\n",
       " 'music',\n",
       " 'weekli',\n",
       " 'wot',\n",
       " 'dude',\n",
       " 'de',\n",
       " 'drop',\n",
       " 'valid',\n",
       " 'alright',\n",
       " 'invit',\n",
       " 'saw',\n",
       " 'yesterday',\n",
       " 'doubl',\n",
       " 'trip',\n",
       " 'food',\n",
       " 'ltdecimalgt',\n",
       " 'oso',\n",
       " 'head',\n",
       " '500',\n",
       " '150p',\n",
       " 'beauti',\n",
       " 'attempt',\n",
       " 'lei',\n",
       " 'search',\n",
       " 'deliveri',\n",
       " 'close',\n",
       " 'busi',\n",
       " 'player',\n",
       " 'yr',\n",
       " 'open',\n",
       " 'si',\n",
       " 'hold',\n",
       " 'hot',\n",
       " 'haf',\n",
       " 'friendship',\n",
       " 'either',\n",
       " 'sch',\n",
       " 'moneysym100',\n",
       " 'wife',\n",
       " 'onlin',\n",
       " 'brother',\n",
       " 'ard',\n",
       " 'mom',\n",
       " 'second',\n",
       " 'bonu',\n",
       " 'caus',\n",
       " 'address',\n",
       " 'inform',\n",
       " 'complet',\n",
       " 'stori',\n",
       " 'nite',\n",
       " 'wid',\n",
       " 'club',\n",
       " 'full',\n",
       " 'tot',\n",
       " 'moneysym500',\n",
       " 'sae',\n",
       " 'famili',\n",
       " 'togeth',\n",
       " 'goin',\n",
       " '8007',\n",
       " 'sad',\n",
       " 'forget',\n",
       " 'moneysym5000',\n",
       " 'isnt',\n",
       " 'old',\n",
       " 'match',\n",
       " 'believ',\n",
       " 'touch',\n",
       " 'noe',\n",
       " 'ring',\n",
       " 'oki',\n",
       " 'reason',\n",
       " 'huh',\n",
       " 'land',\n",
       " 'listen',\n",
       " 'train',\n",
       " 'email',\n",
       " 'murder',\n",
       " 'treat',\n",
       " 'aft',\n",
       " 'fri',\n",
       " 'took',\n",
       " 'privat',\n",
       " 'dog',\n",
       " 'everyon',\n",
       " 'content',\n",
       " 'studi',\n",
       " 'gr8',\n",
       " 'awesom',\n",
       " 'break',\n",
       " 'die',\n",
       " 'wil',\n",
       " 'coz',\n",
       " 'unsubscrib',\n",
       " '86688',\n",
       " 'eve',\n",
       " 'mum',\n",
       " 'rite',\n",
       " 'anyon',\n",
       " 'caller',\n",
       " 'congrat',\n",
       " 'move',\n",
       " 'download',\n",
       " 'prob',\n",
       " 'statement',\n",
       " 'expir',\n",
       " 'age',\n",
       " 'fanci',\n",
       " 'compani',\n",
       " 'angri',\n",
       " '750',\n",
       " 'park',\n",
       " 'choos',\n",
       " 'card',\n",
       " 'sister',\n",
       " 'valentin',\n",
       " 'current',\n",
       " 'simpl',\n",
       " 'tsc',\n",
       " 'neva',\n",
       " 'pub',\n",
       " 'laugh',\n",
       " 'sell',\n",
       " 'valu',\n",
       " '100',\n",
       " 'news',\n",
       " 'tho',\n",
       " 'tomo',\n",
       " 'seem',\n",
       " 'follow',\n",
       " 'lucki',\n",
       " 'ta',\n",
       " '12hr',\n",
       " 'bday',\n",
       " 'bank',\n",
       " 'worth',\n",
       " 'found',\n",
       " 'sort',\n",
       " 'forward',\n",
       " 'mine',\n",
       " 'whatev',\n",
       " 'knw',\n",
       " 'parent',\n",
       " 'alon',\n",
       " 'auction',\n",
       " 'avail',\n",
       " 'joke',\n",
       " 'winner',\n",
       " 'pobox',\n",
       " 'ha',\n",
       " 'smth',\n",
       " 'saturday',\n",
       " 'pass',\n",
       " 'song',\n",
       " 'save',\n",
       " 'oper',\n",
       " 'ticket',\n",
       " 'friday',\n",
       " 'uncl',\n",
       " 'unredeem',\n",
       " 'identifi',\n",
       " 'type',\n",
       " 'hard',\n",
       " 'log',\n",
       " 'boytoy',\n",
       " 'colleg',\n",
       " 'bill',\n",
       " 'exam',\n",
       " 'secret',\n",
       " 'anytim',\n",
       " 'far',\n",
       " 'fone',\n",
       " 'mobileupd8',\n",
       " 'moneysym250',\n",
       " '10',\n",
       " 'welcom',\n",
       " 'kind',\n",
       " 'visit',\n",
       " 'outsid',\n",
       " '8',\n",
       " 'sun',\n",
       " 'sit',\n",
       " 'gd',\n",
       " 'parti',\n",
       " 'surpris',\n",
       " 'crazi',\n",
       " 'camcord',\n",
       " 'cut',\n",
       " 'youll',\n",
       " 'rain',\n",
       " 'gone',\n",
       " 'hit',\n",
       " 'mu',\n",
       " 'nt',\n",
       " 'ltd',\n",
       " 'wit',\n",
       " 'carlo',\n",
       " 'mrng',\n",
       " 'oredi',\n",
       " 'case',\n",
       " 'congratul',\n",
       " 'light',\n",
       " 'return',\n",
       " '150pmsg',\n",
       " 'project',\n",
       " 'bout',\n",
       " 'th',\n",
       " 'cum',\n",
       " 'nope',\n",
       " 'pretti',\n",
       " 'sea',\n",
       " 'fast',\n",
       " 'drug',\n",
       " 'wasnt',\n",
       " 'wkli',\n",
       " '12',\n",
       " 'hungri',\n",
       " 'confirm',\n",
       " 'whole',\n",
       " 'comput',\n",
       " 'moneysym350',\n",
       " 'ni8',\n",
       " 'spend',\n",
       " 'youv',\n",
       " 'wed',\n",
       " 'cours',\n",
       " 'darlin',\n",
       " 'goodmorn',\n",
       " 'ga',\n",
       " '10p',\n",
       " 'meant',\n",
       " 'fix',\n",
       " 'cd',\n",
       " 'unlimit',\n",
       " 'interest',\n",
       " 'jay',\n",
       " 'rock',\n",
       " 'ad',\n",
       " 'ten',\n",
       " 'suppos',\n",
       " 'differ',\n",
       " 'scream',\n",
       " 'remov',\n",
       " 'term',\n",
       " 'cs',\n",
       " 'kid',\n",
       " 'snow',\n",
       " 'opt',\n",
       " 'sex',\n",
       " 'b4',\n",
       " 'gal',\n",
       " 'understand',\n",
       " 'wrong',\n",
       " 'promis',\n",
       " 'link',\n",
       " 'turn',\n",
       " 'catch',\n",
       " 'usual',\n",
       " 'almost',\n",
       " 'correct',\n",
       " 'etc',\n",
       " 'hee',\n",
       " '0800',\n",
       " 'shower',\n",
       " 'mah',\n",
       " 'felt',\n",
       " 'quiz',\n",
       " 'tire',\n",
       " 'wine',\n",
       " 'joy',\n",
       " 'march',\n",
       " 'side',\n",
       " 'tel',\n",
       " 'fr',\n",
       " '87066',\n",
       " 'dnt',\n",
       " 'singl',\n",
       " 'bslvyl',\n",
       " 'lost',\n",
       " 'figur',\n",
       " 'moment',\n",
       " 'st',\n",
       " 'motorola',\n",
       " 'coupl',\n",
       " 'ass',\n",
       " 'pm',\n",
       " 'fight',\n",
       " 'savamob',\n",
       " 'sub',\n",
       " 'within',\n",
       " '2003',\n",
       " '800',\n",
       " 'marri',\n",
       " 'yar',\n",
       " 'area',\n",
       " 'paper',\n",
       " 'knew',\n",
       " 'least',\n",
       " 'earlier',\n",
       " 'nyt',\n",
       " 'film',\n",
       " 'chennai',\n",
       " 'tht',\n",
       " 'fren',\n",
       " 'w',\n",
       " 'freemsg',\n",
       " 'reward',\n",
       " 'eh',\n",
       " 'nation',\n",
       " 'eg',\n",
       " 'cheer',\n",
       " 'crave',\n",
       " 'hospit',\n",
       " 'wow',\n",
       " 'complimentari',\n",
       " 'xx',\n",
       " 'load',\n",
       " 'askd',\n",
       " 'direct',\n",
       " 'activ',\n",
       " 'safe',\n",
       " 'hell',\n",
       " 'mr',\n",
       " 'connect',\n",
       " 'semest',\n",
       " 'bcoz',\n",
       " 'laptop',\n",
       " 'blue',\n",
       " 'swing',\n",
       " 'normal',\n",
       " 'christma',\n",
       " 'via',\n",
       " '1000',\n",
       " '150',\n",
       " 'ago',\n",
       " 'chikku',\n",
       " 'seen',\n",
       " 'slow',\n",
       " 'rental',\n",
       " 'rent',\n",
       " 'ipod',\n",
       " 'remind',\n",
       " 'gym',\n",
       " 'darren',\n",
       " 'an',\n",
       " 'eye',\n",
       " 'store',\n",
       " 'ugh',\n",
       " 'extra',\n",
       " 'photo',\n",
       " 'truth',\n",
       " 'fill',\n",
       " 'support',\n",
       " 'grin',\n",
       " 'luck',\n",
       " 'difficult',\n",
       " 'john',\n",
       " 'father',\n",
       " 'comp',\n",
       " 'usf',\n",
       " 'request',\n",
       " 'copi',\n",
       " 'comin',\n",
       " 'abiola',\n",
       " 'stand',\n",
       " 'loan',\n",
       " 'page',\n",
       " 'txting',\n",
       " 'sometim',\n",
       " 'muz',\n",
       " 'deal',\n",
       " 'orchard',\n",
       " 'kate',\n",
       " 'regist',\n",
       " 'teach',\n",
       " 'expect',\n",
       " 'lover',\n",
       " 'disturb',\n",
       " 'wana',\n",
       " 'sim',\n",
       " 'somebodi',\n",
       " 'small',\n",
       " 'discount',\n",
       " 'india',\n",
       " 'doin',\n",
       " 'hmm',\n",
       " 'silent',\n",
       " 'ladi',\n",
       " 'warm',\n",
       " 'clean',\n",
       " 'door',\n",
       " 'noon',\n",
       " 'idea',\n",
       " 'fall',\n",
       " 'whenev',\n",
       " 'heard',\n",
       " 'frm',\n",
       " 'cancel',\n",
       " 'fantasi',\n",
       " 'fact',\n",
       " 'slowli',\n",
       " 'hr',\n",
       " 'nah',\n",
       " 'callertun',\n",
       " 'press',\n",
       " 'info',\n",
       " 'wap',\n",
       " 'england',\n",
       " 'sick',\n",
       " 'oop',\n",
       " 'situat',\n",
       " 'forev',\n",
       " 'short',\n",
       " 'recent',\n",
       " 'rpli',\n",
       " 'repres',\n",
       " 'gave',\n",
       " 'men',\n",
       " 'apart',\n",
       " 'quot',\n",
       " 'app',\n",
       " 'del',\n",
       " 'lovabl',\n",
       " 'pray',\n",
       " 'wast',\n",
       " 'trust',\n",
       " 'rs',\n",
       " 'sign',\n",
       " 'road',\n",
       " 'kick',\n",
       " 'admir',\n",
       " 'deep',\n",
       " 'hmv',\n",
       " 'stupid',\n",
       " 'somewher',\n",
       " 'pete',\n",
       " 'record',\n",
       " 'immedi',\n",
       " 'access',\n",
       " 'weed',\n",
       " 'met',\n",
       " 'ex',\n",
       " 'woke',\n",
       " 'mm',\n",
       " 'yep',\n",
       " 'voic',\n",
       " 'ldn',\n",
       " 'ure',\n",
       " 'style',\n",
       " 'monday',\n",
       " 'water',\n",
       " 'near',\n",
       " 'opinion',\n",
       " 'less',\n",
       " 'member',\n",
       " 'across',\n",
       " 'cheap',\n",
       " 'em',\n",
       " 'ho',\n",
       " 'gap',\n",
       " 'fantast',\n",
       " 'glad',\n",
       " 'summer',\n",
       " 'gettin',\n",
       " 'reveal',\n",
       " 'poor',\n",
       " 'asap',\n",
       " 'otherwis',\n",
       " 'ntt',\n",
       " 'possibl',\n",
       " '10pmin',\n",
       " 'convey',\n",
       " 'regard',\n",
       " 'doctor',\n",
       " 'who',\n",
       " 'energi',\n",
       " 'nobodi',\n",
       " 'write',\n",
       " 'share',\n",
       " 'natur',\n",
       " 'excus',\n",
       " 'med',\n",
       " 'empti',\n",
       " 'cup',\n",
       " 'std',\n",
       " 'moneysym900',\n",
       " '11',\n",
       " 'bless',\n",
       " 'serious',\n",
       " 'mark',\n",
       " 'boss',\n",
       " 'flight',\n",
       " 'sunshin',\n",
       " 'soni',\n",
       " 'lazi',\n",
       " 'lect',\n",
       " 'becom',\n",
       " 'hmmm',\n",
       " 'lift',\n",
       " 'especi',\n",
       " 'mrt',\n",
       " 'appreci',\n",
       " 'street',\n",
       " 'flirt',\n",
       " 'definit',\n",
       " 'unless',\n",
       " 'teas',\n",
       " 'rose',\n",
       " 'sport',\n",
       " 'accept',\n",
       " 'moneysym200',\n",
       " 'rest',\n",
       " 'power',\n",
       " '11mth',\n",
       " 'merri',\n",
       " 'round',\n",
       " 'urself',\n",
       " 'basic',\n",
       " 'bluetooth',\n",
       " 'refer',\n",
       " 'mistak',\n",
       " 'kinda',\n",
       " 'result',\n",
       " 'optout',\n",
       " 'depend',\n",
       " 'meh',\n",
       " 'self',\n",
       " 'hotel',\n",
       " 'hurri',\n",
       " 'indian',\n",
       " 'xy',\n",
       " 'king',\n",
       " 'add',\n",
       " 'subscrib',\n",
       " 'no1',\n",
       " 'wwwgetzedcouk',\n",
       " 'aint',\n",
       " 'bid',\n",
       " 'chariti',\n",
       " 'tampa',\n",
       " 'user',\n",
       " 'mid',\n",
       " 'sale',\n",
       " 'wear',\n",
       " 'moneysym800',\n",
       " 'hiya',\n",
       " 'f',\n",
       " 'digit',\n",
       " 'p',\n",
       " 'deliv',\n",
       " 'mode',\n",
       " 'other',\n",
       " 'bb',\n",
       " 'total',\n",
       " '2nite',\n",
       " 'arriv',\n",
       " 'list',\n",
       " 'thinkin',\n",
       " 'flag',\n",
       " 'colleagu',\n",
       " 'tear',\n",
       " 'entitl',\n",
       " 'anymor',\n",
       " 'sunday',\n",
       " '87077',\n",
       " 'pizza',\n",
       " 'clear',\n",
       " 'quick',\n",
       " 'learn',\n",
       " 'roommat',\n",
       " 'letter',\n",
       " 'youd',\n",
       " 'nigeria',\n",
       " 'ice',\n",
       " 'cinema',\n",
       " 'spent',\n",
       " 'pleasur',\n",
       " 'troubl',\n",
       " 'coffe',\n",
       " 'ave',\n",
       " 'inc',\n",
       " '2004',\n",
       " 'bother',\n",
       " 'bak',\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# определим клюяевые признаки, по которым будем определять принадлежность к классу\n",
    "\n",
    "word_features = [i[0] for i in all_words.most_common(1500)]\n",
    "# word_features[-1]\n",
    "word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "got\n",
      "n\n",
      "great\n",
      "wat\n",
      "e\n",
      "world\n",
      "point\n",
      "avail\n",
      "crazi\n",
      "bugi\n",
      "la\n",
      "cine\n"
     ]
    }
   ],
   "source": [
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "    return features\n",
    "\n",
    "features = find_features(processed[0])\n",
    "for key, value in features.items():\n",
    "    if value==True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = zip(processed, Y)\n",
    "\n",
    "# define a seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(list(messages))\n",
    "\n",
    "# call find_features function for each SMS message\n",
    "featuresets = [(find_features(text), label) for (text, label) in zip(processed, Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# split the data into training and testing datasets\n",
    "# training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=seed)\n",
    "training, testing = featuresets[:4500], featuresets[4500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500\n",
      "1072\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 98.32089552238806\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
    "\n",
    "# train the model on the training data\n",
    "model.train(training)\n",
    "\n",
    "# and test on the testing dataset!\n",
    "accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "print(\"SVC Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors Accuracy: 93.28358208955224\n",
      "Decision Tree Accuracy: 97.10820895522389\n",
      "Random Forest Accuracy: 98.0410447761194\n",
      "Logistic Regression Accuracy: 98.32089552238806\n",
      "SGD Classifier Accuracy: 98.0410447761194\n",
      "Naive Bayes Accuracy: 97.48134328358209\n",
      "SVM Linear Accuracy: 98.32089552238806\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Define models to train\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_clf',\n",
       " '_encoder',\n",
       " '_make_probdist',\n",
       " '_vectorizer',\n",
       " 'classify',\n",
       " 'classify_many',\n",
       " 'labels',\n",
       " 'prob_classify',\n",
       " 'prob_classify_many',\n",
       " 'train']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: -1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-8ceb2e3e1c35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "nltk_model.classify(df[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble methods - Voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1))\n",
    "nltk_ensemble.train(training)\n",
    "# accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "# print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
